{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import chainer\n",
    "#from chainer import Variable\n",
    "#from chainer import datasets, iterators, optimizers\n",
    "from chainer import Chain\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "#from chainer import training\n",
    "#from chainer.training import extensions\n",
    "from chainer.dataset import convert\n",
    "from chainer.datasets import TupleDataset\n",
    "\n",
    "def get_mnist(n_train=100, n_test=100, n_dim=1, with_label=True, classes=None):\n",
    "    \"\"\"\n",
    "\n",
    "    :param n_train: nr of training examples per class\n",
    "    :param n_test: nr of test examples per class\n",
    "    :param n_dim: 1 or 3 (for convolutional input)\n",
    "    :param with_label: whether or not to also provide labels\n",
    "    :param classes: if not None, then it selects only those classes, e.g. [0, 1]\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    train_data, test_data = chainer.datasets.get_mnist(ndim=n_dim, withlabel=with_label)\n",
    "\n",
    "    if not classes:\n",
    "        classes = np.arange(10)\n",
    "    n_classes = len(classes)\n",
    "\n",
    "    if with_label:\n",
    "\n",
    "        for d in range(2):\n",
    "\n",
    "            if d==0:\n",
    "                data = train_data._datasets[0]\n",
    "                labels = train_data._datasets[1]\n",
    "                n = n_train\n",
    "            else:\n",
    "                data = test_data._datasets[0]\n",
    "                labels = test_data._datasets[1]\n",
    "                n = n_test\n",
    "\n",
    "            for i in range(n_classes):\n",
    "                lidx = np.where(labels == classes[i])[0][:n]\n",
    "                if i==0:\n",
    "                    idx = lidx\n",
    "                else:\n",
    "                    idx = np.hstack([idx,lidx])\n",
    "\n",
    "            L = np.concatenate([i*np.ones(n) for i in np.arange(n_classes)]).astype('int32')\n",
    "\n",
    "            if d==0:\n",
    "                train_data = TupleDataset(data[idx],L)\n",
    "            else:\n",
    "                test_data = TupleDataset(data[idx],L)\n",
    "\n",
    "    else:\n",
    "\n",
    "        tmp1, tmp2 = chainer.datasets.get_mnist(ndim=n_dim, withlabel=True)\n",
    "\n",
    "        for d in range(2):\n",
    "\n",
    "            if d == 0:\n",
    "                data = train_data\n",
    "                labels = tmp1._datasets[1]\n",
    "                n = n_train\n",
    "            else:\n",
    "                data = test_data\n",
    "                labels = tmp2._datasets[1]\n",
    "                n = n_test\n",
    "\n",
    "            for i in range(n_classes):\n",
    "                lidx = np.where(labels == classes[i])[0][:n]\n",
    "                if i == 0:\n",
    "                    idx = lidx\n",
    "                else:\n",
    "                    idx = np.hstack([idx, lidx])\n",
    "\n",
    "            if d == 0:\n",
    "                train_data = data[idx]\n",
    "            else:\n",
    "                test_data = data[idx]\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "batchsize = 32\n",
    "n_units = 10\n",
    "n_epochs = 50  # not much is visible with 20, so we upped it to 50\n",
    "max_hid_layers = 3\n",
    "\n",
    "class CNN(Chain):\n",
    "    def __init__(self, activation=\"sigmoid\", dropout=False, batch_norm=False):\n",
    "        \n",
    "        self.activation = activation\n",
    "        self.dropout = dropout\n",
    "        self.batch_norm = batch_norm\n",
    "        \n",
    "        super(CNN, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.conv = L.Convolution2D(in_channels=1, \n",
    "                                        out_channels=5, \n",
    "                                        ksize=5, \n",
    "                                        stride=1, \n",
    "                                        pad=0)\n",
    "            if self.batch_norm: \n",
    "                self.bn = L.BatchNormalization(5)\n",
    "            self.fc = L.Linear(None, 10)\n",
    "        \n",
    "            \n",
    "    def __call__(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.dropout:\n",
    "            x = F.dropout(x, ratio=0.4)\n",
    "        if self.activation == \"relu\":\n",
    "            h = F.relu(x)\n",
    "        #elif self.activation == \"sigmoid\":\n",
    "        h = F.sigmoid(x)\n",
    "        h = F.max_pooling_2d(h, 2, 2)\n",
    "        if self.batch_norm: \n",
    "            h = self.bn(h)\n",
    "        with chainer.using_config('train', True):\n",
    "            if self.dropout:\n",
    "                h = F.dropout(h, ratio=0.4)\n",
    "            return self.fc(h)\n",
    "        return F.softmax(self.fc(h))\n",
    "    \n",
    "# train, test = chainer.datasets.get_mnist()\n",
    "train_conv, test_conv = get_mnist(n_dim=3)\n",
    "\n",
    "train_iter_conv = chainer.iterators.SerialIterator(train_conv, batchsize)\n",
    "test_iter_conv = chainer.iterators.SerialIterator(test_conv, batchsize,\n",
    "                                             repeat=False, \n",
    "                                             shuffle=False)\n",
    "\n",
    "train_count_conv = len(train_conv)\n",
    "test_count_conv = len(test_conv)\n",
    "\n",
    "def train_CNN(train_iter_conv, test_iter_conv, model, optimizer, n_epochs, train_count, test_count):\n",
    "    \n",
    "    train_iter_conv.reset()\n",
    "    test_iter_conv.reset()\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    \n",
    "    sum_accuracy = 0\n",
    "    sum_loss = 0\n",
    "    \n",
    "    while train_iter_conv.epoch < n_epochs:\n",
    "        batch = train_iter_conv.next()\n",
    "\n",
    "        x_array, t_array = convert.concat_examples(batch, -1)\n",
    "        x = chainer.Variable(x_array)\n",
    "        t = chainer.Variable(t_array)\n",
    "        optimizer.update(model, x, t)\n",
    "        sum_loss += float(model.loss.data) * len(t.data)\n",
    "        sum_accuracy += float(model.accuracy.data) * len(t.data)\n",
    "\n",
    "        if train_iter_conv.is_new_epoch:\n",
    "            print('epoch: ', train_iter_conv.epoch)\n",
    "            train_loss = sum_loss / train_count\n",
    "            train_losses.append(train_loss)\n",
    "            print('train mean loss: {}, accuracy: {}'.format(\n",
    "                  train_loss, sum_accuracy / train_count))\n",
    "\n",
    "            sum_accuracy = 0\n",
    "            sum_loss = 0\n",
    "            model.predictor.train = False\n",
    "            for batch in test_iter_conv:\n",
    "                x_array, t_array = convert.concat_examples(batch, -1)\n",
    "                x = chainer.Variable(x_array)\n",
    "                t = chainer.Variable(t_array)\n",
    "                loss = model(x, t)\n",
    "                sum_loss += float(loss.data) * len(t.data)\n",
    "                sum_accuracy += float(model.accuracy.data) * len(t.data)\n",
    "\n",
    "            test_iter_conv.reset()\n",
    "            model.predictor.train = True\n",
    "            test_loss = sum_loss / test_count\n",
    "            test_losses.append(test_loss)\n",
    "            print('test mean  loss: {}, accuracy: {}'.format(\n",
    "                  test_loss, sum_accuracy / test_count))\n",
    "            sum_accuracy = 0\n",
    "            sum_loss = 0\n",
    "            \n",
    "        return train_losses, test_losses\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#%%\n",
    "net = CNN()\n",
    "model = L.Classifier(net)    \n",
    "train_iter_conv.reset()\n",
    "test_iter_conv.reset()\n",
    "\n",
    "net.cleargrads()\n",
    "optimizer = chainer.optimizers.SGD()\n",
    "optimizer.setup(model)\n",
    "train_losses_def, test_losses_def = train_CNN(train_iter_conv, test_iter_conv, \n",
    "                                                model, optimizer, n_epochs, train_count_conv, test_count_conv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
