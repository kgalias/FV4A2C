{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import chainer\n",
    "from chainer import Variable\n",
    "from chainer import Chain\n",
    "import chainer.functions as F\n",
    "import chainer.links as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nZkkOkVmSQ2SW5BCZJf0fosFRdAxkxb8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f49d0f17978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "class RandomAgent(object):\n",
    "    \"\"\"The world's simplest agent!\"\"\"\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        return self.action_space.sample()\n",
    "\n",
    "env = gym.make('Breakout-v0')\n",
    "print(env.unwrapped.get_action_meanings())\n",
    "\n",
    "n_iter = 5\n",
    "agent = RandomAgent(env.action_space)\n",
    "obs = env.reset()  # reset environment and agent\n",
    "reward = None\n",
    "done = False\n",
    "R = []\n",
    "\n",
    "for step in range(n_iter):\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    action = agent.act(obs, reward, done)\n",
    "    _obs, reward, done, _ = env.step(action)\n",
    "    obs = _obs\n",
    "    R.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n",
      "Box(210, 160, 3)\n",
      "[0 0 0]\n",
      "[255 255 255]\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "print(env.observation_space.low[0][0])\n",
    "print(env.observation_space.high[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ActorCritic(chainer.Chain):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        pass\n",
    "        return pi_out, v_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5, 2, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_shape = (2,3)\n",
    "a = np.zeros([2 + 1, 5] + list(obs_shape))\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def A2CAgent():\n",
    "    \n",
    "    def __init__(model, n_steps):\n",
    "        \n",
    "        self.model = model\n",
    "        self.step = 0\n",
    "        self.n_steps = n_steps\n",
    "    \n",
    "    def _compute_reward(self):\n",
    "      \n",
    "    def _compute_returns(self, next_value):  # ?\n",
    "        self.returns[-1] = next_value\n",
    "        for i in reversed(range(self.update_steps)):\n",
    "            self.returns[i] = self.rewards[i] + \\\n",
    "                self.gamma * self.returns[i + 1] * self.masks[i]\n",
    "    \n",
    "    def _reset_storage(self):\n",
    "        \n",
    "#         self.states = self.xp.zeros(\n",
    "#             [self.update_steps + 1, self.num_processes] + list(obs_shape),\n",
    "#             dtype='f')\n",
    "#         self.actions = self.xp.zeros(\n",
    "#             [self.update_steps, self.num_processes] + list(action_shape),\n",
    "#             dtype=action.dtype)\n",
    "#         self.rewards = self.xp.zeros(\n",
    "#             (self.update_steps, self.num_processes, 1), dtype='f')\n",
    "#         self.value_preds = self.xp.zeros(\n",
    "#             (self.update_steps + 1, self.num_processes, 1), dtype='f')\n",
    "#         self.returns = self.xp.zeros(\n",
    "#             (self.update_steps + 1, self.num_processes, 1), dtype='f')\n",
    "\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.value_preds = []\n",
    "        self.returns = []\n",
    "    \n",
    "    def update(self):\n",
    "        pass\n",
    "    \n",
    "    def act_and_train(self, state, reward, done):\n",
    "        \n",
    "#         statevar = self.batch_states([state], self.xp, self.phi)[0]\n",
    "        \n",
    "        if self.steps == 0:\n",
    "#             pout, _ = self.model.pi_and_v(statevar[0:1])\n",
    "#             action = pout.sample().data\n",
    "            self._reset_storage()\n",
    "        \n",
    "#         self.rewards[self.t - self.t_start -1] \\\n",
    "#             = self.xp.array(reward, dtype=self.xp.float32)\n",
    "#         self.states[self.t - self.t_start] = statevar\n",
    "        self.states.append(state)\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "        if self_step % n_steps == 0:\n",
    "            self.update()\n",
    "            \n",
    "        p_out, value = self.model(state)\n",
    "        action = p_out.sample().data    \n",
    "        \n",
    "#         self.actions[self.t - self.t_start] \\\n",
    "#             = action.reshape([-1] + list(self.action_shape))\n",
    "#         self.value_preds[self.t - self.t_start] = value.data\n",
    "        self.actions.append(action) \n",
    "        self.value_preds[self.t - self.t_start] = value.data     \n",
    "\n",
    "        self.steps += 1\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def act(self, obs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QFunction(chainer.Chain):\n",
    "\n",
    "    def __init__(self, obs_size, n_actions, n_hidden_channels=50):\n",
    "        super().__init__()\n",
    "        with self.init_scope():\n",
    "            self.l0 = L.Linear(obs_size, n_hidden_channels)\n",
    "            self.l1 = L.Linear(n_hidden_channels, n_hidden_channels)\n",
    "            self.l2 = L.Linear(n_hidden_channels, n_actions)\n",
    "\n",
    "    def __call__(self, x, test=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (ndarray or chainer.Variable): An observation\n",
    "            test (bool): a flag indicating whether it is in test mode\n",
    "        \"\"\"\n",
    "        h = F.tanh(self.l0(x))\n",
    "        h = F.tanh(self.l1(h))\n",
    "        return chainerrl.action_value.DiscreteActionValue(self.l2(h))\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "q_func = QFunction(obs_size, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import chainer \n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import chainerrl\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from chainer import cuda\n",
    "\n",
    "import datetime\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "env = gym.make('Breakout-v0')\n",
    "obs = env.reset()\n",
    "\n",
    "print(\"observation space   : {}\".format(env.observation_space))\n",
    "print(\"action space        : {}\".format(env.action_space))\n",
    "\n",
    "action = env.action_space.sample()\n",
    "obs, r, done, info = env.step(action)\n",
    "class QFunction(chainer.Chain):\n",
    "def __init__(self,obs_size, n_action):\n",
    "    super(QFunction, self).__init__(\n",
    "        l1=L.Convolution2D(obs_size, 4, ksize=2,pad=1),#210x160\n",
    "        bn1=L.BatchNormalization(4),\n",
    "        l2=L.Convolution2D(4, 4, ksize=2,pad=1),#105x80\n",
    "        bn2=L.BatchNormalization(4),\n",
    "        #l3=L.Convolution2D(64, 64, ksize=2, pad=1),#100x100\n",
    "        #bn3=L.BatchNormalization(64),\n",
    "        #l4=L.Convolution2D(64, 3, ksize=2,pad=1),#50x50\n",
    "       # bn4=L.BatchNormalization(3),\n",
    "\n",
    "        l5=L.Linear(972, 512),\n",
    "        out=L.Linear(512, n_action, initialW=np.zeros((n_action, 512), dtype=np.float32))\n",
    "    )\n",
    "\n",
    "def __call__(self, x, test=False):\n",
    "\n",
    "    h1=F.relu(self.bn1(self.l1(x)))\n",
    "    h2=F.max_pooling_2d(F.relu(self.bn2(self.l2(h1))),2)\n",
    "    #h3=F.relu(self.bn3(self.l3(h2)))\n",
    "    #h4=F.max_pooling_2d(F.relu(self.bn4(self.l4(h3))),2)\n",
    "    #print h4.shape\n",
    "\n",
    "    return chainerrl.action_value.DiscreteActionValue(self.out(self.l5(h2)))\n",
    "\n",
    "n_action = env.action_space.n\n",
    "obs_size = env.observation_space.shape[0] #(210,160,3)\n",
    "q_func = QFunction(obs_size, n_action)\n",
    "\n",
    "optimizer = chainer.optimizers.Adam(eps=1e-2)\n",
    "optimizer.setup(q_func)\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "explorer = chainerrl.explorers.ConstantEpsilonGreedy(\n",
    "epsilon=0.2, random_action_func=env.action_space.sample)\n",
    "\n",
    "replay_buffer = chainerrl.replay_buffer.ReplayBuffer(capacity=10 ** 6)\n",
    "\n",
    "phi = lambda x: x.astype(np.float32, copy=False)\n",
    "agent = chainerrl.agents.DoubleDQN(\n",
    "q_func, optimizer, replay_buffer, gamma, explorer,\n",
    "minibatch_size=4, replay_start_size=100, update_interval=10,\n",
    "target_update_interval=10, phi=phi)\n",
    "\n",
    "last_time = datetime.datetime.now()\n",
    "n_episodes = 10000\n",
    "for i in range(1, n_episodes + 1):\n",
    "obs = env.reset()\n",
    "\n",
    "reward = 0\n",
    "done = False\n",
    "R = 0\n",
    "\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = agent.act_and_train(obs, reward)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "\n",
    "\n",
    "    if reward != 0:\n",
    "        R += reward\n",
    "\n",
    "elapsed_time = datetime.datetime.now() - last_time\n",
    "print('episode:', i, \n",
    "      'reward:', R,\n",
    "     )\n",
    "last_time = datetime.datetime.now()\n",
    "\n",
    "if i % 100 == 0:\n",
    "    filename = 'agent_Breakout' + str(i)\n",
    "    agent.save(filename)\n",
    "\n",
    "agent.stop_episode_and_train(obs, reward, done)\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer freezing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 weights: [[[[ 1.]]]]\n",
      "l1 weigths: [[ 1.  1.  1.  1.]]\n",
      "conv1 grad [[[[ 0.19457532]]]]\n",
      "after update...\n",
      "conv1 weights: [[[[ 1.]]]]\n",
      "l1 weights: [[ 0.99955279  0.99984294  0.99892896  0.99972951]]\n"
     ]
    }
   ],
   "source": [
    "class CNN(chainer.Chain):\n",
    "    \n",
    "    def __init__(self, train=True):\n",
    "        super(CNN, self).__init__(\n",
    "            conv1 = L.Convolution2D(1, 1, 1, initialW=np.array([[[[1]]]])) ,\n",
    "            l1 = L.Linear(None, 1, initialW=np.array([[1,1,1,1]]))  \n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.conv1(x))\n",
    "        h = F.sigmoid(self.l1(h))\n",
    "        return h\n",
    "\n",
    "\n",
    "net = CNN()\n",
    "net.cleargrads()\n",
    "optimizer = chainer.optimizers.SGD()\n",
    "optimizer.setup(net)\n",
    "\n",
    "optimizer.setup(net)\n",
    "net.conv1.W.update_rule.enabled = False\n",
    "# net.conv1.disable_update()  # also works\n",
    "\n",
    "result = net(np.random.random((1,1,2,2)).astype(np.float32))\n",
    "print(\"conv1 weights:\",net.conv1.W.data)\n",
    "print(\"l1 weigths:\",net.l1.W.data)\n",
    "loss = F.mean_absolute_error(result,np.array([[0.1]],dtype=np.float32))\n",
    "loss.backward()\n",
    "optimizer.update()\n",
    "print(\"conv1 grad\",net.conv1.W.grad)\n",
    "print(\"after update...\")\n",
    "print(\"conv1 weights:\",net.conv1.W.data)\n",
    "print(\"l1 weights:\",net.l1.W.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gym]",
   "language": "python",
   "name": "conda-env-gym-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
