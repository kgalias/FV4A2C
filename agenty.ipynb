{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import chainer\n",
    "from chainer import Variable\n",
    "from chainer import Chain\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.optimizers import RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTMxsy/CYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe1953c1a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "\n",
    "class RandomAgent(object):\n",
    "    \"\"\"The world's simplest agent!\"\"\"\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        return self.action_space.sample()\n",
    "\n",
    "env = gym.make('Breakout-v0')\n",
    "print(env.unwrapped.get_action_meanings())\n",
    "\n",
    "n_iter = 5\n",
    "agent = RandomAgent(env.action_space)\n",
    "\n",
    "env = gym.make('Breakout-v0')\n",
    "obs = env.reset()  # reset environment and agent\n",
    "reward = None\n",
    "done = False\n",
    "R = []\n",
    "\n",
    "for step in range(n_iter):\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    action = agent.act(obs, reward, done)\n",
    "    _obs, reward, done, _ = env.step(action)\n",
    "    obs = _obs\n",
    "    R.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe195357898>]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wE66I2RkSYDSAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe19535c240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.cumsum(R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n",
      "Box(210, 160, 3)\n",
      "[0 0 0]\n",
      "[255 255 255]\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "print(env.observation_space.low[0][0])\n",
    "print(env.observation_space.high[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.low.size\n",
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLP(Chain):\n",
    "    \n",
    "    def __init__(self, obs_size, n_units=64):\n",
    "        super(MLP, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.l0 = L.Linear(obs_size, n_units)\n",
    "            self.l1 = L.Linear(n_units, 2 * n_units)\n",
    "            self.l2 = L.Linear(2 * n_units, n_units)\n",
    "            \n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.l0(x))\n",
    "        h = F.relu(self.l1(h))\n",
    "        return F.relu(self.l2(h))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActorCritic(Chain):\n",
    "    \n",
    "    def __init__(self, shared_model, n_actions):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        with self.init_scope():\n",
    "            self.shared_model = shared_model\n",
    "            \n",
    "            self.actor = L.Linear(None, n_actions)\n",
    "            self.critic = L.Linear(None, 1)\n",
    "    \n",
    "    \n",
    "    def __call__(self, x):\n",
    "        h = self.shared_model(x)\n",
    "    \n",
    "        pi = F.softmax(self.actor(h))\n",
    "        v = self.critic(h)\n",
    "        return pi, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AC = ActorCritic(shared_model=MLP(obs_size), n_actions=n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(variable([[ 0.26258507,  0.26294535,  0.23586331,  0.23860621]]),\n",
       " variable([[-0.15567851]]))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AC(frame_preprocessor(obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 5, 2, 3) (1, 3, 5, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "obs_shape = (2,3)\n",
    "a = np.zeros([2 + 1, 5] + list(obs_shape))\n",
    "print(a.shape, a.reshape([-1] + list(a.shape)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_compute_returns illustration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 13.45990546  12.58576309  11.702791    10.8109       9.91         9.        ]\n"
     ]
    }
   ],
   "source": [
    "update_steps = 5\n",
    "gamma = 0.99\n",
    "returns = np.zeros(update_steps + 1)\n",
    "rewards = [1, 1, 1, 1, 1]\n",
    "next_val = 9\n",
    "returns[-1] = next_val \n",
    "for i in reversed(range(update_steps)):\n",
    "    returns[i] = rewards[i] + gamma * returns[i + 1]\n",
    "\n",
    "print(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros((1,4))\n",
    "a.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CAgent(object):\n",
    "    \n",
    "    def __init__(self, model, optimizer, n_steps, gamma, phi=lambda x: x, \n",
    "                 pi_loss_coef=1.0, v_loss_coef=0.5,\n",
    "                ):\n",
    "        \n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.phi = phi\n",
    "    \n",
    "        self.step = 0\n",
    "        self.start_step = 0\n",
    "    \n",
    "    def _compute_returns(self, next_value):\n",
    "        \n",
    "        self.returns[-1] = next_value\n",
    "        for i in reversed(range(self.update_steps)):\n",
    "            self.returns[i] = self.rewards[i] + \\\n",
    "                self.gamma * self.returns[i + 1]\n",
    "    \n",
    "    def _reset_storage(self, state):\n",
    "        \n",
    "#         self.states = self.xp.zeros(\n",
    "#             [self.update_steps + 1, self.num_processes] + list(obs_shape),\n",
    "#             dtype='f')\n",
    "#         self.actions = self.xp.zeros(\n",
    "#             [self.update_steps, self.num_processes] + list(action_shape),\n",
    "#             dtype=action.dtype)\n",
    "#         self.rewards = self.xp.zeros(\n",
    "#             (self.update_steps, self.num_processes, 1), dtype='f')\n",
    "#         self.value_preds = self.xp.zeros(\n",
    "#             (self.update_steps + 1, self.num_processes, 1), dtype='f')\n",
    "#         self.returns = self.xp.zeros(\n",
    "#             (self.update_steps + 1, self.num_processes, 1), dtype='f')\n",
    "\n",
    "        self.states = np.zeros((self.n_steps + 1,) + state.shape)\n",
    "        self.action_log_probs = np.zeros(self.n_steps)\n",
    "        self.rewards = np.zeros(self.n_steps)\n",
    "        self.value_preds = np.zeros(self.n_steps + 1)\n",
    "        self.returns = np.zeros(self.n_steps + 1)\n",
    "    \n",
    "    def update(self):\n",
    "        #print(len(self.states), len(self.actions), len(self.rewards), len(self.value_preds), len(self.returns))\n",
    "        \n",
    "        _, next_value = self.model(self.states[-1])\n",
    "        next_value = next_value.data\n",
    "\n",
    "        self._compute_returns(next_value)\n",
    "#         pout, values = \\\n",
    "#             self.model.pi_and_v(chainer.Variable(\n",
    "#                 self.states[:-1].reshape([-1] + list(self.obs_shape))))\n",
    "        p_out, values = self.model(chainer.Variable(self.states[:-1]))\n",
    "\n",
    "#         actions = chainer.Variable(\n",
    "#             self.actions.reshape([-1] + list(self.action_shape)))\n",
    "#         dist_entropy = F.mean(pout.entropy)\n",
    "#         action_log_probs = pout.log_prob(actions)\n",
    "        # dist_entropy = ???\n",
    "        \n",
    "#         values = values.reshape(self.update_steps, self.num_processes, 1)\n",
    "#         action_log_probs = action_log_probs.reshape(\n",
    "#             self.update_steps, self.num_processes, 1)\n",
    "#         advantages = chainer.Variable(self.returns[:-1]) - values\n",
    "        advantages = chainer.Variable(self.returns[:-1]) - values\n",
    "        value_loss = F.mean(advantages * advantages)\n",
    "        action_loss = - F.mean(chainer.Variable(advantages.data) * action_log_probs)\n",
    "        \n",
    "        self.model.cleargrads()\n",
    "\n",
    "        (value_loss * self.v_loss_coef +\n",
    "         action_loss * self.pi_loss_coef).backward()\n",
    "         # - dist_entropy * self.entropy_coeff).backward()\n",
    "\n",
    "        self.optimizer.update()\n",
    "        self.states[0] = self.states[-1]\n",
    "\n",
    "        # Update stats\n",
    "#         self.average_actor_loss += (\n",
    "#             (1 - self.average_actor_loss_decay) *\n",
    "#             (float(action_loss.data) - self.average_actor_loss))\n",
    "#         self.average_value += (\n",
    "#             (1 - self.average_value_decay) *\n",
    "#             (float(value_loss.data) - self.average_value))\n",
    "#         self.average_entropy += (\n",
    "#             (1 - self.average_entropy_decay) *\n",
    "#             (float(dist_entropy.data) - self.average_entropy))\n",
    "        self.start_step = self.step\n",
    "    \n",
    "    \n",
    "    def act_and_train(self, state, reward, done):\n",
    "\n",
    "#         statevar = self.batch_states([state], self.xp, self.phi)[0]\n",
    "        state = self.phi(state)\n",
    "        \n",
    "        if self.step == 0:\n",
    "#             pout, _ = self.model.pi_and_v(statevar[0:1])\n",
    "#             action = pout.sample().data\n",
    "            p_out, _ = self.model(state)\n",
    "            action = np.random.choice(n_actions, p=p_out.data.flatten())\n",
    "            self._reset_storage(state)\n",
    "        \n",
    "#         self.rewards[self.t - self.t_start -1] \\\n",
    "#             = self.xp.array(reward, dtype=self.xp.float32)\n",
    "#         self.states[self.t - self.t_start] = statevar\n",
    "    \n",
    "        # self.rewards.append(reward)\n",
    "        self.rewards[self.step - self.start_step] = np.array(reward, dtype=np.float32)\n",
    "        # self.states.append(state)\n",
    "        self.states[self.step - self.start_step] = state\n",
    "\n",
    "        if self.step - self.start_step == self.n_steps:\n",
    "            self.update()\n",
    "            \n",
    "        p_out, value = self.model(state)\n",
    "        action = np.random.choice(n_actions, p=p_out.data.flatten())\n",
    "        # action = env.action_space.sample()  # random\n",
    "        \n",
    "#         self.actions[self.t - self.t_start] \\\n",
    "#             = action.reshape([-1] + list(self.action_shape))\n",
    "#         self.value_preds[self.t - self.t_start] = value.data\n",
    "        self.action_log_probs = F.log(p_out)\n",
    "        # self.actions.append(action) \n",
    "        self.value_preds[self.step - self.start_step] = value.data\n",
    "        # self.value_preds.append(0)\n",
    "\n",
    "        self.step += 1\n",
    "            \n",
    "        return action\n",
    "        # return env.action_space.sample()  # random\n",
    "    \n",
    "    def act(self, obs):\n",
    "\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample an action given probabilites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 2, 3, 3, 1, 3, 3])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_actions = 4\n",
    "probs = [0.1, 0.2, 0.3, 0.4]\n",
    "\n",
    "np.random.choice(4, 10, p=probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for axis 0 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-179-238c6612405d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#plt.imshow(env.render(mode='rgb_array'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_and_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0m_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_obs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-177-df51563a4477>\u001b[0m in \u001b[0;36mact_and_train\u001b[0;34m(self, state, reward, done)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# self.rewards.append(reward)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_step\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0;31m# self.states.append(state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_step\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 5 is out of bounds for axis 0 with size 5"
     ]
    }
   ],
   "source": [
    "env = gym.make('Breakout-v0')\n",
    "print(env.action_space)\n",
    "obs_size = env.observation_space.low.size\n",
    "n_actions = env.action_space.n\n",
    "n_steps = 5\n",
    "\n",
    "model = ActorCritic(shared_model=MLP(obs_size), n_actions=n_actions)\n",
    "optimizer = chainer.optimizers.RMSprop()\n",
    "optimizer.setup(model)\n",
    "\n",
    "frame_preprocessor = lambda x: (x.astype(np.float32) / 255).reshape([-1] + list(x.shape))\n",
    "agent = A2CAgent(model=model, optimizer=optimizer, gamma=0.99, n_steps=n_steps, phi=frame_preprocessor)\n",
    "\n",
    "# print(\"states actions rewards value_preds returns\")\n",
    "\n",
    "obs = env.reset()\n",
    "reward = 0\n",
    "done = False\n",
    "R = []\n",
    "\n",
    "for _ in range(50):\n",
    "    #plt.imshow(env.render(mode='rgb_array'))\n",
    "    action = agent.act_and_train(obs, reward, done)\n",
    "    _obs, reward, done, _ = env.step(action)\n",
    "    obs = _obs\n",
    "    R.append(reward)\n",
    "\n",
    "plt.plot(np.cumsum(R))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_episodes = 200\n",
    "max_episode_len = 200\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = env.reset()\n",
    "    reward = 0\n",
    "    done = False\n",
    "    R = 0  # return (sum of rewards)\n",
    "    t = 0  # time step\n",
    "    while not done and t < max_episode_len:\n",
    "        # Uncomment to watch the behaviour\n",
    "        # env.render()\n",
    "        action = agent.act_and_train(obs, reward)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        R += reward\n",
    "        t += 1\n",
    "    if i % 10 == 0:\n",
    "        print('episode:', i,\n",
    "              'R:', R,\n",
    "              'statistics:', agent.get_statistics())\n",
    "    agent.stop_episode_and_train(obs, reward, done)\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "gym.undo_logger_setup()  # Turn off gym's default logger settings\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='')\n",
    "\n",
    "chainerrl.experiments.train_agent_with_evaluation(\n",
    "    agent, env,\n",
    "    steps=2000,           # Train the agent for 2000 steps\n",
    "    eval_n_runs=10,       # 10 episodes are sampled for each evaluation\n",
    "    max_episode_len=200,  # Maximum length of each episodes\n",
    "    eval_interval=1000,   # Evaluate the agent after every 1000 steps\n",
    "    outdir='result')      # Save everything to 'result' directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QFunction(chainer.Chain):\n",
    "\n",
    "    def __init__(self, obs_size, n_actions, n_hidden_channels=50):\n",
    "        super().__init__()\n",
    "        with self.init_scope():\n",
    "            self.l0 = L.Linear(obs_size, n_hidden_channels)\n",
    "            self.l1 = L.Linear(n_hidden_channels, n_hidden_channels)\n",
    "            self.l2 = L.Linear(n_hidden_channels, n_actions)\n",
    "\n",
    "    def __call__(self, x, test=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (ndarray or chainer.Variable): An observation\n",
    "            test (bool): a flag indicating whether it is in test mode\n",
    "        \"\"\"\n",
    "        h = F.tanh(self.l0(x))\n",
    "        h = F.tanh(self.l1(h))\n",
    "        return chainerrl.action_value.DiscreteActionValue(self.l2(h))\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "q_func = QFunction(obs_size, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import chainer \n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import chainerrl\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from chainer import cuda\n",
    "\n",
    "import datetime\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "\n",
    "env = gym.make('Breakout-v0')\n",
    "obs = env.reset()\n",
    "\n",
    "print(\"observation space   : {}\".format(env.observation_space))\n",
    "print(\"action space        : {}\".format(env.action_space))\n",
    "\n",
    "action = env.action_space.sample()\n",
    "obs, r, done, info = env.step(action)\n",
    "class QFunction(chainer.Chain):\n",
    "def __init__(self,obs_size, n_action):\n",
    "    super(QFunction, self).__init__(\n",
    "        l1=L.Convolution2D(obs_size, 4, ksize=2,pad=1),#210x160\n",
    "        bn1=L.BatchNormalization(4),\n",
    "        l2=L.Convolution2D(4, 4, ksize=2,pad=1),#105x80\n",
    "        bn2=L.BatchNormalization(4),\n",
    "        #l3=L.Convolution2D(64, 64, ksize=2, pad=1),#100x100\n",
    "        #bn3=L.BatchNormalization(64),\n",
    "        #l4=L.Convolution2D(64, 3, ksize=2,pad=1),#50x50\n",
    "       # bn4=L.BatchNormalization(3),\n",
    "\n",
    "        l5=L.Linear(972, 512),\n",
    "        out=L.Linear(512, n_action, initialW=np.zeros((n_action, 512), dtype=np.float32))\n",
    "    )\n",
    "\n",
    "def __call__(self, x, test=False):\n",
    "\n",
    "    h1=F.relu(self.bn1(self.l1(x)))\n",
    "    h2=F.max_pooling_2d(F.relu(self.bn2(self.l2(h1))),2)\n",
    "    #h3=F.relu(self.bn3(self.l3(h2)))\n",
    "    #h4=F.max_pooling_2d(F.relu(self.bn4(self.l4(h3))),2)\n",
    "    #print h4.shape\n",
    "\n",
    "    return chainerrl.action_value.DiscreteActionValue(self.out(self.l5(h2)))\n",
    "\n",
    "n_action = env.action_space.n\n",
    "obs_size = env.observation_space.shape[0] #(210,160,3)\n",
    "q_func = QFunction(obs_size, n_action)\n",
    "\n",
    "optimizer = chainer.optimizers.Adam(eps=1e-2)\n",
    "optimizer.setup(q_func)\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "explorer = chainerrl.explorers.ConstantEpsilonGreedy(\n",
    "epsilon=0.2, random_action_func=env.action_space.sample)\n",
    "\n",
    "replay_buffer = chainerrl.replay_buffer.ReplayBuffer(capacity=10 ** 6)\n",
    "\n",
    "phi = lambda x: x.astype(np.float32, copy=False)\n",
    "agent = chainerrl.agents.DoubleDQN(\n",
    "q_func, optimizer, replay_buffer, gamma, explorer,\n",
    "minibatch_size=4, replay_start_size=100, update_interval=10,\n",
    "target_update_interval=10, phi=phi)\n",
    "\n",
    "last_time = datetime.datetime.now()\n",
    "n_episodes = 10000\n",
    "for i in range(1, n_episodes + 1):\n",
    "obs = env.reset()\n",
    "\n",
    "reward = 0\n",
    "done = False\n",
    "R = 0\n",
    "\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = agent.act_and_train(obs, reward)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "\n",
    "\n",
    "    if reward != 0:\n",
    "        R += reward\n",
    "\n",
    "elapsed_time = datetime.datetime.now() - last_time\n",
    "print('episode:', i, \n",
    "      'reward:', R,\n",
    "     )\n",
    "last_time = datetime.datetime.now()\n",
    "\n",
    "if i % 100 == 0:\n",
    "    filename = 'agent_Breakout' + str(i)\n",
    "    agent.save(filename)\n",
    "\n",
    "agent.stop_episode_and_train(obs, reward, done)\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer freezing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 weights: [[[[ 1.]]]]\n",
      "l1 weigths: [[ 1.  1.  1.  1.]]\n",
      "conv1 grad [[[[ 0.19457532]]]]\n",
      "after update...\n",
      "conv1 weights: [[[[ 1.]]]]\n",
      "l1 weights: [[ 0.99955279  0.99984294  0.99892896  0.99972951]]\n"
     ]
    }
   ],
   "source": [
    "class CNN(chainer.Chain):\n",
    "    \n",
    "    def __init__(self, train=True):\n",
    "        super(CNN, self).__init__(\n",
    "            conv1 = L.Convolution2D(1, 1, 1, initialW=np.array([[[[1]]]])) ,\n",
    "            l1 = L.Linear(None, 1, initialW=np.array([[1,1,1,1]]))  \n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = F.relu(self.conv1(x))\n",
    "        h = F.sigmoid(self.l1(h))\n",
    "        return h\n",
    "\n",
    "\n",
    "net = CNN()\n",
    "net.cleargrads()\n",
    "optimizer = chainer.optimizers.SGD()\n",
    "optimizer.setup(net)\n",
    "\n",
    "optimizer.setup(net)\n",
    "net.conv1.W.update_rule.enabled = False\n",
    "# net.conv1.disable_update()  # also works\n",
    "\n",
    "result = net(np.random.random((1,1,2,2)).astype(np.float32))\n",
    "print(\"conv1 weights:\",net.conv1.W.data)\n",
    "print(\"l1 weigths:\",net.l1.W.data)\n",
    "loss = F.mean_absolute_error(result,np.array([[0.1]],dtype=np.float32))\n",
    "loss.backward()\n",
    "optimizer.update()\n",
    "print(\"conv1 grad\",net.conv1.W.grad)\n",
    "print(\"after update...\")\n",
    "print(\"conv1 weights:\",net.conv1.W.data)\n",
    "print(\"l1 weights:\",net.l1.W.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyTorch tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Discount factor. Model is not very sensitive to this value.\n",
    "GAMMA = .95\n",
    "\n",
    "# LR of 3e-2 explodes the gradients, LR of 3e-4 trains slower\n",
    "LR = 3e-3\n",
    "N_GAMES = 2000\n",
    "\n",
    "# OpenAI baselines uses nstep of 5.\n",
    "N_STEPS = 20\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "N_ACTIONS = 2 # get from env\n",
    "N_INPUTS = 4 # get from env\n",
    "\n",
    "model = ActorCritic()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "finished_games = 0\n",
    "\n",
    "while finished_games < N_GAMES:\n",
    "    states, actions, rewards, dones = [], [], [], []\n",
    "\n",
    "    # Gather training data\n",
    "    for i in range(N_STEPS):\n",
    "        s = Variable(torch.from_numpy(state).float().unsqueeze(0))\n",
    "\n",
    "        action_probs = model.get_action_probs(s)\n",
    "        action = action_probs.multinomial().data[0][0]\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        states.append(state); actions.append(action); rewards.append(reward); dones.append(done)\n",
    "\n",
    "        if done: state = env.reset(); finished_games += 1\n",
    "        else: state = next_state\n",
    "\n",
    "    # Reflect on training data\n",
    "    reflect(states, actions, rewards, dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_actual_state_values(rewards, dones):\n",
    "    R = []\n",
    "    rewards.reverse()\n",
    "\n",
    "    # If we happen to end the set on a terminal state, set next return to zero\n",
    "    if dones[-1] == True: next_return = 0\n",
    "        \n",
    "    # If not terminal state, bootstrap v(s) using our critic\n",
    "    # TODO: don't need to estimate again, just take from last value of v(s) estimates\n",
    "    else: \n",
    "        s = torch.from_numpy(states[-1]).float().unsqueeze(0)\n",
    "        next_return = model.get_state_value(Variable(s)).data[0][0] \n",
    "    \n",
    "    # Backup from last state to calculate \"true\" returns for each state in the set\n",
    "    R.append(next_return)\n",
    "    dones.reverse()\n",
    "    for r in range(1, len(rewards)):\n",
    "        if not dones[r]: this_return = rewards[r] + next_return * GAMMA\n",
    "        else: this_return = 0\n",
    "        R.append(this_return)\n",
    "        next_return = this_return\n",
    "\n",
    "    R.reverse()\n",
    "    state_values_true = Variable(torch.FloatTensor(R)).unsqueeze(1)\n",
    "    \n",
    "    return state_values_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reflect(states, actions, rewards, dones):\n",
    "    \n",
    "    # Calculating the ground truth \"labels\" as described above\n",
    "    state_values_true = calc_actual_state_values(rewards, dones)\n",
    "\n",
    "    s = Variable(torch.FloatTensor(states))\n",
    "    action_probs, state_values_est = model.evaluate_actions(s)\n",
    "    action_log_probs = action_probs.log() \n",
    "    \n",
    "    a = Variable(torch.LongTensor(actions).view(-1,1))\n",
    "    chosen_action_log_probs = action_log_probs.gather(1, a)\n",
    "\n",
    "    # This is also the TD error\n",
    "    advantages = state_values_true - state_values_est\n",
    "\n",
    "    entropy = (action_probs * action_log_probs).sum(1).mean()\n",
    "    action_gain = (chosen_action_log_probs * advantages).mean()\n",
    "    value_loss = advantages.pow(2).mean()\n",
    "    total_loss = value_loss - action_gain - 0.0001*entropy\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    nn.utils.clip_grad_norm(model.parameters(), 0.5)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.linear1 = nn.Linear(N_INPUTS, 64)\n",
    "        self.linear2 = nn.Linear(64, 128)\n",
    "        self.linear3 = nn.Linear(128, 64)\n",
    "        \n",
    "        self.actor = nn.Linear(64, N_ACTIONS)\n",
    "        self.critic = nn.Linear(64, 1)\n",
    "    \n",
    "    # In a PyTorch model, you only have to define the forward pass. PyTorch computes the backwards pass for you!\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        x = F.relu(x) \n",
    "        return x\n",
    "    \n",
    "    # Only the Actor head\n",
    "    def get_action_probs(self, x):\n",
    "        x = self(x)\n",
    "        action_probs = F.softmax(self.actor(x))\n",
    "        return action_probs\n",
    "    \n",
    "    # Only the Critic head\n",
    "    def get_state_value(self, x):\n",
    "        x = self(x)\n",
    "        state_value = self.critic(x)\n",
    "        return state_value\n",
    "    \n",
    "    # Both heads\n",
    "    def evaluate_actions(self, x):\n",
    "        x = self(x)\n",
    "        action_probs = F.softmax(self.actor(x))\n",
    "        state_values = self.critic(x)\n",
    "        return action_probs, state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(model):\n",
    "    score = 0\n",
    "    done = False\n",
    "    env = gym.make('CartPole-v0')\n",
    "    state = env.reset()\n",
    "    global action_probs\n",
    "    while not done:\n",
    "        score += 1\n",
    "        s = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        \n",
    "        action_probs = model.get_action_probs(Variable(s))\n",
    "        \n",
    "        _, action_index = action_probs.max(1)\n",
    "        action = action_index.data[0] \n",
    "        next_state, reward, done, thing = env.step(action)\n",
    "        state = next_state\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gym]",
   "language": "python",
   "name": "conda-env-gym-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
